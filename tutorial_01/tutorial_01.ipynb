{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Intro to NLP with spaCy](https://nicschrading.com/project/Intro-to-NLP-with-spaCy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  インストール\n",
    "\n",
    "* [usage](https://spacy.io/docs/usage/)を参照\n",
    "\n",
    "* ModelはEnglishを選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up spaCy\n",
    "from spacy.en import English\n",
    "\n",
    "parser = English()\n",
    "\n",
    "# Test Data\n",
    "multi_sentence = \"There is an art, it says, or rather, a knack to flying.\" \\\n",
    "                 \"The knack lies in learning how to throw yourself at the ground and miss.\" \\\n",
    "                 \"In the beginning the Universe was created. This has made a lot of people \"\\\n",
    "                 \"very angry and been widely regarded as a bad move.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCyは**トークン化**, **文の認識**, **品詞のタグ付け**, **レンマ化(見出し語認識)**, **係り受け解析**, **固有表現抽出** を一度に行うことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文章のパース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 769 There\n",
      "lowercased: 608 there\n",
      "lemma: 608 there\n",
      "shape: 684 Xxxxx\n",
      "prefix: 568 T\n",
      "suffix: 609 ere\n",
      "log probability: -7.277902603149414\n",
      "Brown cluster id: 1918\n",
      "----------------------------------------\n",
      "original: 513 is\n",
      "lowercased: 513 is\n",
      "lemma: 536 be\n",
      "shape: 505 xx\n",
      "prefix: 509 i\n",
      "suffix: 513 is\n",
      "log probability: -4.3297648429870605\n",
      "Brown cluster id: 762\n",
      "----------------------------------------\n",
      "original: 591 an\n",
      "lowercased: 591 an\n",
      "lemma: 591 an\n",
      "shape: 505 xx\n",
      "prefix: 506 a\n",
      "suffix: 591 an\n",
      "log probability: -5.953293800354004\n",
      "Brown cluster id: 3\n",
      "----------------------------------------\n",
      "original: 879 art\n",
      "lowercased: 879 art\n",
      "lemma: 879 art\n",
      "shape: 502 xxx\n",
      "prefix: 506 a\n",
      "suffix: 879 art\n",
      "log probability: -9.778430938720703\n",
      "Brown cluster id: 633\n",
      "----------------------------------------\n",
      "original: 450 ,\n",
      "lowercased: 450 ,\n",
      "lemma: 450 ,\n",
      "shape: 450 ,\n",
      "prefix: 450 ,\n",
      "suffix: 450 ,\n",
      "log probability: -3.3914804458618164\n",
      "Brown cluster id: 4\n",
      "----------------------------------------\n",
      "original: 519 it\n",
      "lowercased: 519 it\n",
      "lemma: 757862 -PRON-\n",
      "shape: 505 xx\n",
      "prefix: 509 i\n",
      "suffix: 519 it\n",
      "log probability: -4.5064496994018555\n",
      "Brown cluster id: 474\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 文章のパースはこれだけでできる\n",
    "# note: ファイル内でspaCyを初めに呼び出すときはモジュールの読み込みに少し時間が掛かる\n",
    "parsed_data = parser(multi_sentence)\n",
    "\n",
    "# トークンを見ていく　\n",
    "# parse_dateをイテレートするだけでOK\n",
    "# それぞれのトークンは多くのプロパティを持ったオブジェクトである\n",
    "# アンダースコアが末尾についたプロパティはトークンの文字列表現\n",
    "# アンダースコアが末尾にないプロパティはSpacyのvocabularyへのインデックス(int)を返す\n",
    "# 確率推定は30億語からのなるコーパスに基づいている\n",
    "# コーパスはSimple Good-Turing法により平滑化されている\n",
    "for i, token in enumerate(parsed_data):\n",
    "    # 文章に出現している形\n",
    "    print(\"original:\", token.orth, token.orth_)\n",
    "    # 小文字\n",
    "    print(\"lowercased:\", token.lower, token.lower_)\n",
    "    # lemma: 見出し形・原形\n",
    "    print(\"lemma:\", token.lemma, token.lemma_)\n",
    "    # shape: \n",
    "    print(\"shape:\", token.shape, token.shape_)\n",
    "    # 単語の先頭のN文字(デフォルトではN=1)\n",
    "    print(\"prefix:\", token.prefix, token.prefix_)\n",
    "    # 単語の末尾のN文字(デフォルトではN=3)\n",
    "    print(\"suffix:\", token.suffix, token.suffix_)\n",
    "    # 確率推定(コーパスでの出現確率?)\n",
    "    print(\"log probability:\", token.prob)\n",
    "    #\n",
    "    print(\"Brown cluster id:\", token.cluster)\n",
    "    print(\"----------------------------------------\")\n",
    "    # 元のチュートリアルでは最初の単語のみ表示させているが、\n",
    "    # ここでは4番目まで表示させてみる\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ここでチュートリアルを脱線して、*Zen of Python*を使って同じことをしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 209155 Beautiful\n",
      "lowercased: 3020 beautiful\n",
      "lemma: 3020 beautiful\n",
      "shape: 684 Xxxxx\n",
      "prefix: 704 B\n",
      "suffix: 1899 ful\n",
      "log probability: -19.579313278198242\n",
      "Brown cluster id: 966\n",
      "Part of Speech: 94 PROPN\n",
      "sentiment: 0.0\n",
      "----------------------------------------\n",
      "original: 513 is\n",
      "lowercased: 513 is\n",
      "lemma: 536 be\n",
      "shape: 505 xx\n",
      "prefix: 509 i\n",
      "suffix: 513 is\n",
      "log probability: -4.3297648429870605\n",
      "Brown cluster id: 762\n",
      "Part of Speech: 98 VERB\n",
      "sentiment: 0.0\n",
      "----------------------------------------\n",
      "original: 761 better\n",
      "lowercased: 761 better\n",
      "lemma: 673 good\n",
      "shape: 515 xxxx\n",
      "prefix: 537 b\n",
      "suffix: 762 ter\n",
      "log probability: -7.226652145385742\n",
      "Brown cluster id: 7658\n",
      "Part of Speech: 82 ADJ\n",
      "sentiment: 0.0\n",
      "----------------------------------------\n",
      "original: 626 than\n",
      "lowercased: 626 than\n",
      "lemma: 626 than\n",
      "shape: 515 xxxx\n",
      "prefix: 503 t\n",
      "suffix: 627 han\n",
      "log probability: -6.372464179992676\n",
      "Brown cluster id: 106\n",
      "Part of Speech: 83 ADP\n",
      "sentiment: 0.0\n",
      "----------------------------------------\n",
      "original: 3173 ugly\n",
      "lowercased: 3173 ugly\n",
      "lemma: 3173 ugly\n",
      "shape: 515 xxxx\n",
      "prefix: 607 u\n",
      "suffix: 3174 gly\n",
      "log probability: -10.173290252685547\n",
      "Brown cluster id: 871\n",
      "Part of Speech: 82 ADJ\n",
      "sentiment: 0.0\n",
      "----------------------------------------\n",
      "original: 453 .\n",
      "lowercased: 453 .\n",
      "lemma: 453 .\n",
      "shape: 453 .\n",
      "prefix: 453 .\n",
      "suffix: 453 .\n",
      "log probability: -3.0729479789733887\n",
      "Brown cluster id: 8\n",
      "Part of Speech: 95 PUNCT\n",
      "sentiment: 0.0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "zen_of_python = \"Beautiful is better than ugly.\"\\\n",
    "                \"Explicit is better than implicit.\"\\\n",
    "                \"Simple is better than complex.\"\\\n",
    "                \"Complex is better than complicated.\"\\\n",
    "                \"Flat is better than nested.\"\\\n",
    "                \"Sparse is better than dense.\"\\\n",
    "                \"Readability counts.\"\\\n",
    "                \"Special cases aren't special enough to break the rules.\"\\\n",
    "                \"Although practicality beats purity.\"\\\n",
    "                \"Errors should never pass silently.\"\\\n",
    "                \"Unless explicitly silenced.\"\\\n",
    "                \"In the face of ambiguity, refuse the temptation to guess.\"\\\n",
    "                \"There should be one-- and preferably only one --obvious way to do it.\"\\\n",
    "                \"Although that way may not be obvious at first unless you're Dutch.\"\\\n",
    "                \"Now is better than never.\"\\\n",
    "                \"Although never is often better than *right* now.\"\\\n",
    "                \"If the implementation is hard to explain, it's a bad idea.\"\\\n",
    "                \"If the implementation is easy to explain, it may be a good idea.\"\\\n",
    "                \"Namespaces are one honking great idea -- let's do more of those!\"\\\n",
    "\n",
    "parse_zop = parser(zen_of_python)\n",
    "\n",
    "for i, token in enumerate(parse_zop):\n",
    "    print(\"original:\", token.orth, token.orth_)\n",
    "    print(\"lowercased:\", token.lower, token.lower_)\n",
    "    print(\"lemma:\", token.lemma, token.lemma_)\n",
    "    print(\"shape:\", token.shape, token.shape_)\n",
    "    print(\"prefix:\", token.prefix, token.prefix_)\n",
    "    print(\"suffix:\", token.suffix, token.suffix_)\n",
    "    print(\"log probability:\", token.prob)\n",
    "    print(\"Brown cluster id:\", token.cluster)\n",
    "    # 品詞\n",
    "    print(\"Part of Speech:\", token.pos, token.pos_)\n",
    "    # ネガポジ値?\n",
    "    print(\"sentiment:\", token.sentiment)\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PROPN: properNoun(固有名詞 主語になっているため固有名詞として扱われている?)\n",
    "* ADP: Preposition and postposition(接置詞)\n",
    "\n",
    "品詞とネガポジ値のAPIも試してみた。\n",
    "ネガポジ値はすべて0.0になってしまっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文章に着目してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sents`プロパティは`spans`を返す\n",
    "\n",
    "`spans`は元の文章へのインデックスを持っている\n",
    "\n",
    "それぞれのインデックスの要素は`token`で表される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an art, it says, or rather, a knack to flying.\n",
      "The knack lies in learning how to throw yourself at the ground and miss.\n",
      "In the beginning the Universe was created.\n",
      "This has made a lot of people very angry and been widely regarded as a bad move.\n"
     ]
    }
   ],
   "source": [
    "sents = []\n",
    "for span in parsed_data.sents:\n",
    "    # それぞれのspanの開始から終了を表示\n",
    "    # 戻り値である文章のトークンを join() で結合している\n",
    "    sent = ''.join(parsed_data[i].string for i in range(span.start, span.end)).strip()\n",
    "    sents.append(sent)\n",
    "    \n",
    "for sent in sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最初の文章の各単語の品詞を見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There ADV\n",
      "is VERB\n",
      "an DET\n",
      "art NOUN\n",
      ", PUNCT\n",
      "it PRON\n",
      "says VERB\n",
      ", PUNCT\n",
      "or CCONJ\n",
      "rather ADV\n",
      ", PUNCT\n",
      "a DET\n",
      "knack NOUN\n",
      "to ADP\n",
      "flying NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for span in parsed_data.sents:\n",
    "    sent = [parsed_data[i] for i in range(span.start, span.end)]\n",
    "    break\n",
    "\n",
    "for token in sent:\n",
    "    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次の例文の依存関係を見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det boy [] []\n",
      "boy nsubj ran ['The'] ['with']\n",
      "with prep boy [] []\n",
      "the det dog [] []\n",
      "spotted amod dog [] []\n",
      "dog nsubj ran ['the', 'spotted'] []\n",
      "quickly advmod ran [] []\n",
      "ran ROOT ran ['boy', 'dog', 'quickly'] ['after', '.']\n",
      "after prep ran [] ['firetruck']\n",
      "the det firetruck [] []\n",
      "firetruck pobj after ['the'] []\n",
      ". punct ran [] []\n"
     ]
    }
   ],
   "source": [
    "example = \"The boy with the spotted dog quickly ran after the firetruck.\"\n",
    "parsed_ex = parser(example)\n",
    "\n",
    "for token in parsed_ex:\n",
    "    # shown as: original token, dependency tag, head word, left dependents, right dependents\n",
    "    print(token.orth_, \n",
    "          token.dep_, \n",
    "          token.head.orth_, \n",
    "          [t.orth_ for t in token.lefts], \n",
    "          [t.orth_ for t in token.rights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* original token: 元のトークン\n",
    "* dependency tag: 依存タグ。構文的な依存関係。\n",
    "    * [Stanford typed dependencies manual](https://nlp.stanford.edu/software/dependencies_manual.pdf)\n",
    "    * [東邦大学 - テキストマイニング/Stanfordパーザーの細かい点](http://pepper.is.sci.toho-u.ac.jp/index.php?%A5%CE%A1%BC%A5%C8%2F%A5%C6%A5%AD%A5%B9%A5%C8%A5%DE%A5%A4%A5%CB%A5%F3%A5%B0%2FStanford%A5%D1%A1%BC%A5%B6%A1%BC%A4%CE%BA%D9%A4%AB%A4%A4%C5%C0)\n",
    "* head word:[token.head](https://spacy.io/docs/api/token#head)トークンの構文親\n",
    "* left dependents\n",
    "* right dependents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次の例文の固有表現を見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "'s (not an entity)\n",
      "stocks (not an entity)\n",
      "dropped (not an entity)\n",
      "dramatically (not an entity)\n",
      "after (not an entity)\n",
      "the (not an entity)\n",
      "death (not an entity)\n",
      "of (not an entity)\n",
      "Steve PERSON\n",
      "Jobs PERSON\n",
      "in (not an entity)\n",
      "October DATE\n",
      ". (not an entity)\n",
      "-------------- entities only ---------------\n",
      "380 ORG Apple\n",
      "377 PERSON Steve Jobs\n",
      "387 DATE October\n"
     ]
    }
   ],
   "source": [
    "example = \"Apple's stocks dropped dramatically after the death of Steve Jobs in October.\"\n",
    "parsed_ex = parser(example)\n",
    "for token in parsed_ex:\n",
    "    #　token_ent_type_: 固有表現タイプ\n",
    "    print(token.orth_, token.ent_type_ if token.ent_type_ != \"\" else \"(not an entity)\")\n",
    "    \n",
    "\n",
    "print(\"-------------- entities only ---------------\")\n",
    "# Doc.ents で固有表現だけを取得できる\n",
    "ents = parsed_ex.ents\n",
    "for entity in ents:\n",
    "    print(entity.label, entity.label_, ' '.join(t.orth_ for t in entity) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCyはemoticonやネット上の表現の処理を試みるよう訓練されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol NOUN lol\n",
      "that ADJ that\n",
      "is VERB be\n",
      "rly ADV rly\n",
      "funny ADJ funny\n",
      ":) PUNCT :)\n",
      "This DET this\n",
      "is VERB be\n",
      "gr8 VERB gr8\n",
      "i PRON i\n",
      "rate VERB rate\n",
      "it PRON -PRON-\n",
      "8/8 NUM 8/8\n",
      "! PUNCT !\n",
      "! PUNCT !\n",
      "! PUNCT !\n"
     ]
    }
   ],
   "source": [
    "messy_data = \"lol that is rly funny :) This is gr8 i rate it 8/8!!!\"\n",
    "parsed_data = parser(messy_data)\n",
    "\n",
    "for token in parsed_data:\n",
    "    print(token.orth_, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いい感じです。\n",
    "\n",
    "`gr8`のトークンの失敗に注意。`gr8`は`great`、つまり形容詞だが動詞となっている。\n",
    "\n",
    "また`lol`も名詞ではなく、感動詞のようなもの。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCyには単語のベクトル表現がビルトインされている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元のチュートリアルでは `w.has_repvec` となっており、そのまま実行すると\n",
    "```\n",
    "AttributeError: 'spacy.lexeme.Lexeme' object has no attribute 'has_repvec'\n",
    "```\n",
    "と例外が発生する。\n",
    "\n",
    "現在のAPIでは`Lexeme.has_vector`となっている。\n",
    "[Lexeme.has_vector](https://spacy.io/docs/api/lexeme#vector)\n",
    "\n",
    "---\n",
    "\n",
    "`parser.vocab['NASA']`は` word vector`を持っていないため、類似値計算において\n",
    "```\n",
    "RuntimeWarning: invalid value encountered in float_scalars\n",
    "```\n",
    "が発生する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7681語\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nao/Python/env/spaCy_tutorial/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "nasa = parser.vocab['NASA']\n",
    "\n",
    "# コサイン類似度を求めるlambda式\n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "# parserのvocabularyにある単語を、小文字の形式で取得する\n",
    "all_words = list({w for w in parser.vocab \n",
    "                  if w.has_vector and w.orth_.islower() and w.lower_ != 'nasa'})\n",
    "\n",
    "# vocaularyから取得した単語数\n",
    "print('{length}語'.format(length=len(all_words)))\n",
    "\n",
    "print(nasa.has_vector)\n",
    "\n",
    "# NASAとの類似度順にソートする\n",
    "all_words.sort(key=lambda w: cosine(w.vector, nasa.vector))\n",
    "all_words.reverse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `NASA`では例外が発生するので`rock`で試してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock\n",
      "rocks\n",
      "punk\n",
      "band\n",
      "pop\n",
      "bands\n",
      "blues\n",
      "indie\n",
      "music\n",
      "metal\n"
     ]
    }
   ],
   "source": [
    "rock = parser.vocab['rock']\n",
    "\n",
    "all_words.sort(key=lambda w: cosine(w.vector, rock.vector))\n",
    "all_words.reverse()\n",
    "for word in all_words[:10]:\n",
    "    print(word.orth_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 男性は`王`になる。では女性は何になるのか、という類推をしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen\n",
      "kings\n",
      "princess\n"
     ]
    }
   ],
   "source": [
    "king = parser.vocab['king']\n",
    "man = parser.vocab['man']\n",
    "woman = parser.vocab['woman']\n",
    "\n",
    "result = king.vector - man.vector + woman.vector\n",
    "\n",
    "all_words = list({w for w in parser.vocab\n",
    "                  if w.has_vector and w.orth_.islower()})\n",
    "\n",
    "for word in ['king', 'man', 'woman']:\n",
    "    all_words.remove(word)\n",
    "    \n",
    "all_words.sort(key=lambda w: cosine(w.vector, result))\n",
    "all_words.reverse()\n",
    "\n",
    "for word in all_words[:3]:   \n",
    "    print(word.orth_)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`王` - `男性` + `女性` は `女王`　となる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
